The approach in \cite{zutshi2014multiple} uses the forward simulation
map to explore a fixed number of tuples in the reachability relation
$\reach{t,\vu}$ specified by a user-provided budget.  In general, the
budget required to adequately explore the transition space requires
exploring an exponential (in the dimension of $\HybridStates$) number
of such tuples. This can cause undue burden in terms of computation
time as well as storage.  The central idea in this paper is to {\em
summarize} several tuples as a single affine map obtained using linear
regression.  In other words, we can replace several ``proximal''
tuples $(\vx,\vx')$ in $\reach{t,u}$ by an affine map of the form
$\vx' = A\vx + B\vu + \delta$, where $A$ and $B$ are matrices and
$\delta$ is a worst-case error estimate.

In statistics, linear regression is the problem of finding an affine
function or \textit{predictor}, which can `best' summarize the
relationship between the given set of observed input $\x$ and output
$\x'$ vectors. The notion of `best' is formally captured using a loss
function which can be non-linear. We use the commonly used loss
function \textit{ordinary least squares} (OLS) for this presentation.

\mypara{Ordinary Least Squares (OLS)}
For rest of the section, we elide the discrete mode from the hybrid
state of the system, and only focus on the continuous state. Note that
we can do this because of our assumption that the system is a
switched-mode dynamical system.  Suppose we have $N$ tuples of the
form $(\x,\x')$ in the reachability relation that we have explored,
where $\x\in\reals^n$ and $\x'\in\reals^n$. Then, we wish to learn an
affine function described as 
\begin{equation}
\label{eq:affinemap}
\x' = A\x + B
\end{equation}
Here, $A$ is a $n\times n$ matrix, and $B$ is a $n\times 1$ matrix.


If $N>n$, which is the case in the current context of \textit{finding
the best fit}, the problem is over-determined: there are more
equations than unknowns. Hence, a single affine function cannot be
found which satisfies the equation \eqref{eq:affinemap}. Instead, we
need to find the `best' choice for $\vec{a}$ and $b$. This choice is
formally defined using a loss function. For the case of simple linear
regression or OLS, the loss function is the sum of squares of the
errors in prediction.  The task is then to determine $A$ and $B$, such
that the least square error of the affine predictor is minimized for
the given data set.

\begin{equation}
    \operatornamewithlimits{argmin}_{A,B}\displaystyle\sum_{i=1}^{N}{\left(\x' - (A\x + B)\right)^2}
\label{reg}
\end{equation}

To ease the presentation, we can rewrite the above as a homogeneous
expression by using $\hat{\x} = [\x^T\ 1]^T$ and replacing $A$ and $B$ by the vector
$\vec{A} = [A\ B]$. The equation~\ref{reg} now becomes
\[
\operatornamewithlimits{argmin}_{\vec{A}}\displaystyle\sum_{i=1}^{N}{(\x' - \vec{A}^T \hat{\x})^2}
\]

The solution of OLS can be analytically computed as the Moore-Penrose
pseudo-inverse: $\vec{A} = (X^TX)^{-1}X^T X'$, where $X$ is the matrix
representing the horizontal stacking of all $\hat{\x}$ and $X'$
represents the stacking of all $\x'$ values.  The details can be found
in standard texts on learning and statistics
\cite{friedman2001elements}. Alternative methods based on gradient
descent can also be used to find a suitable $A$ matrix.

In this fashion, we can we can use OLS to approximate its trajectories
of system $\System$ at fixed time step $\Delta$ by a discrete map $\x'
= \vec{A}\hat{\x}$.

% where
% $A = \begin{bmatrix}\vec{a}_1^T \\ \vdots \\ \vec{a}_n^T
% \end{bmatrix}$ and $\vb = \begin{bmatrix}b_1 \\ \vdots \\ b_n
% \end{bmatrix}$. 

We note that linear regression tools are often able to provide an
$n\times 1$ vector $\vec{\delta}$ of error intervals, where the
$i^{th}$ entry of the form $[{\delta_i}_{\min},{\delta_i}_{\max}]$
indicates the best and worst possible error in computation of the
$i^{th}$ state in $\x'$.


Affine maps are poor approximations for arbitrary non-linear
functions. Hence, we use a collection of affine maps to approximate
the local behaviors (in state-space) of the system $\System$. This
results in a piece wise approximation, as described in the next
section.


% \mypara{Regression Analysis}

% In statistics, regression is the problem of finding a
% \textit{predictor}, which can suitably predict the relationship
% between the given set of observed input $\x$ and output $\y$ vectors.
% In other words, assuming that $\y$ depends on $\x$, regression
% strategies find either a parameterized or a non-parameterized
% prediction function to explain the dependence. We now discuss simple
% linear regression, which is parametric in nature and searches for an
% affine predictor. It is also called \textit{ordinary least squares}.

% \mypara{Ordinary Least Squares (OLS)}

% Let the data set be comprised of $N$ input and output pairs $(\x,
% y)$, where $\x\in\reals^n$ and $y\in\reals$. If $N>n$, which is
% the case in the current context of \textit{finding the best fit}, the
% problem is over-determined; there are more equations than
% unknowns. Hence, a single affine function cannot be found which
% satisfies the equation $\forall i\in\{1\ldots N\}. y_i = \vec{a}^T\x_i + b$. Instead, we
% need to find the `best' choice for $\vec{a}$ and $b$. This is formally
% defined using a loss function. For the case of simple linear
% regression or OLS, the loss function is the sum of squares of the
% errors in prediction.  The task is then to determine the vector of
% coefficients $\vec{a}$ and an offset constant $b$, such that the least
% square error of the affine predictor is minimized for the given data
% set.
% \begin{equation}
%     \operatornamewithlimits{argmin}_{\vec{a}, b}\displaystyle\sum_{i=1}^{N}{\left(\y_i - (\vec{a}^T\x_i + b)\right)^2}
% \label{reg}
% \end{equation}
% To ease the presentation, we can rewrite the above as a homogeneous
% expression by augmenting $\x$ by $\hat{\x} = \begin{bmatrix}\x \\ 1\end{bmatrix}$ and replacing $\vec{a}$ and $b$ by the vector
% $\vec{ab} = \begin{bmatrix}\vec{a} \\ b \end{bmatrix}$. The equation~\ref{reg} now becomes
% \[
%     \operatornamewithlimits{argmin}_{\vec{ab}}\displaystyle\sum_{i=1}^{N}{(\y_i - \vec{ab}^T \hat{\x}_i)^2}
% \]
% The solution of OLS can be analytically computed as
% \[ Ab = (X^TX)^{-1}X^T\y\]
% where $X$ is the matrix representing the horizontal stacking of all
% $\hat{\x}$.
% The details can be found in several texts on learning and statistics
% \cite{friedman2001elements}.

% Given a time invariant dynamical system $\x' = \simulate(\x, \Delta)$,
% where $\x\in\reals^n$ we can use OLS to approximate its trajectories
% at fixed time step $\Delta$ by a discrete map $\x' = A\x + \vb$, where
% $A = \begin{bmatrix}\vec{a}_1^T \\ \vdots \\ \vec{a}_n^T \end{bmatrix}$ and $\vb =
% \begin{bmatrix}b_1 \\ \vdots \\ b_n \end{bmatrix}$. This map is a
%     global relational model for the system. The data set for the
%     regression is a set of pairs $\setof{(\x,\x') | \x' =
%     \simulate(\x, \Delta)}$ and can be generated on demand. Another
%     set can be generated to compute the error $\delta$ as an interval
%     of vectors, where each element is an interval $\delta_i \in
%     [\delta_{min}, \delta_{max}]$.
%
%     Affine maps are poor
%     approximations for arbitrary non-linear functions. Hence, we use a
%     collection of affine maps to approximate the local behaviors (in
%     state-space) of the system $\simulate$. This results in a piece
%     wise approximation, as described in the next section.



% \subsection{Relational Abstractions}
% For a hybrid automaton model, they can summarize the continuous
% dynamics of each mode using a binary relation over continuous states.
% The resulting relations are timeless (independent) and hence valid for
% all time as long as the mode invariant is satisfied. The relations
% take the general form of $R(\x,\x') \bowtie 0$, where $\bowtie$
% represents one of the relational operators $=, \ge, \le, <, >$. For
% example, an abstraction that captures the monotonicity with respect to
% time for the differential equation $\dot{x} = 2$ is $x' > x$. The
% abstraction capturing the relation between the set of ODEs: $\dot{x} =
% 2$, $\dot{y} = 5$, is $5(x' - x) = 2(y' - y)$.

%%  Similar to timeless relational abstractions, time-aware relational
%%  abstractions \cite{mover2013time} construct binary relations between
%%  the current state $\x$ of the system and any future reachable state
%%  $\x'$. The difference lies with the latter also constructing relations
%%  between the current time $t$ and any future time $t'$. This is
%%  achieved by a case by case analysis of the eigen structure of the
%%  matrix $A$ (of an affine ODE). Separate abstractions are used for the
%%  case of linear systems with constant rate, real eigen values and
%%  complex eigenvalues.

