The approach in \cite{zutshi2014multiple} uses the forward simulation
map to explore a fixed number of tuples in the reachability relation
$\reach{t,\vu}$ specified by a user-provided budget.  In general, the
budget required to adequately explore the transition space requires
exploring an exponential (in the dimension of $\HybridStates$) number
of such tuples. This can cause undue burden in terms of computation
time as well as storage.  The central idea in this paper is to {\em
summarize} several tuples as a single map obtained using regression.
In other words, we can replace several ``proximal'' tuples
$(\vx,\vx')$ in $\reach{t,u}$ by a map $F$ of the form $\vx' = F(\vx)
+ \delta$, where $\delta$ is a worst-case error estimate. The form of
$F$ depends on the kernel used for regression. For the special case of
linear regression, the kernel is affine and the resulting map can be
represented by a matrix $A$ as $\vx' = A\vx + \delta$.

In statistics, regression is the problem of finding an affine
function or \textit{predictor}, which can `best' summarize the
relationship between the given set of observed input $\x$ and output
$\x'$ vectors. The notion of `best' is formally captured using a loss
function which can be non-linear. We use the commonly used loss
function \textit{ordinary least squares} (OLS) for this presentation.

If $N>n$, which is the case in the current context of \textit{finding
the best fit}, the problem is over-determined: there are more
equations than unknowns. Hence, a single affine function cannot be
found which satisfies the equation \eqref{eq:affinemap}. Instead, we
need to find the `best' choice for $\vec{a}$ and $b$. This choice is
formally defined using a loss function. For the case of simple linear
regression or OLS, the loss function is the sum of squares of the
errors in prediction.  The task is then to determine $A$ and $B$, such
that the least square error of the affine predictor is minimized for
the given data set.


\mypara{Ordinary Least Squares (OLS)}
For rest of the section, we elide the discrete mode from the hybrid
state of the system, and only focus on the continuous state. Note that
we can do this because of our assumption that the system is a
switched-mode dynamical system.  Suppose we have $N$ tuples of the
form $(\x,\x')$ in the reachability relation that we have explored,
where $\x\in\reals^n$ and $\x'\in\reals^n$. Then, we wish to learn an
affine function described as $\x' = A\x + B$.
Here, $A$ is a $n\times n$ matrix, and $B$ is a $n\times 1$ matrix.
The details can be found in standard texts on learning and
statistics~\cite{friedman2001elements}.

In this fashion, we can we can use OLS to approximate its trajectories
of system $\System$ at fixed time step $\Delta$ by a discrete map $\x'
= \vec{A}\hat{\x}$.

We note that linear regression tools are often able to provide an
$n\times 1$ vector $\vec{\delta}$ of error intervals, where the
$i^{th}$ entry of the form $[{\delta_i}_{\min},{\delta_i}_{\max}]$
indicates the best and worst possible error in computation of the
$i^{th}$ state in $\x'$.

Affine maps are poor approximations for arbitrary non-linear
functions. Hence, we use a collection of affine maps to approximate
the local behaviors (in state-space) of the system $\System$. This
results in a piece wise approximation, as described in the next
section.

\begin{example}
    For the~\exref{vdp}, for every abstract relation between two cells
    $C, C'$, linear regression analysis is performed on the respective
    set of trajectory segments, and the affine relation is computed.
    \figref{vdp-abs-paths} shows the cells and the trajectory segments, which
    are part of the data sets constructed using the $1$-relational
    modeling. Against each cell, its unique identifier (integer
    co-ordinate) is mentioned.
\end{example}
