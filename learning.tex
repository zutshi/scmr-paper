The approach in \cite{zutshi2014multiple} uses the forward simulation
map to explore a fixed number of tuples in the reachability relation
$\reach{t,\vu}$ specified by a user-provided budget.  In general, the
budget required to adequately explore the transition space requires
exploring an exponential (in the dimension of $\HybridStates$) number
of such tuples. This can cause undue burden in terms of computation
time as well as storage.  The central idea in this paper is to {\em
summarize} several tuples as a single map obtained using parametric
regression.  In other words, we can replace several ``proximal''
tuples $(\vx,\vx')$ in $\reach{t,u}$ by a map $F$ of the form $\vx' =
F(\vx, \vu) + \delta$, where $\delta$ is a worst-case error estimate. The
form of $F$ depends on the kernel used for regression. For the special
case of linear regression, the kernel is affine and the resulting map
can be represented by a matrices $A$ and $B$ as $\vx' = A\vx + B\vu +
\delta$.

In statistics, parametric regression is the problem of finding the
paremters of a function or \textit{predictor}, which can `best'
summarize the relationship between the given set of observed input
$\x$ and output $\x'$ vectors. The model or the kernel for the
predictor is fixed beforehand and the paramters are searched for.  The
notion of `best' is formally captured using a loss function which can
be non-linear. We use the commonly used loss function \textit{ordinary
least squares} (OLS) to fit affine and polynomial kernels of fixed
degree.  ~\footnote{The choice of loss function and regression
analysis affects the quality of the learnt models. As the procss of
selecting an appropriate heuristic is not straightforward, we just use
the simplest one.} OLS defines its loss function as the sum of squares
of the errors in prediction. The task is then to determine the
parameters, such that the least square error of the predictor is
minimized for the given data set.

For rest of the section, we elide the discrete mode from the hybrid
state of the system, and only focus on the continuous state. Note that
we can do this because of our assumption that the system is a
switched-mode dynamical system.  Suppose we have $N$ tuples of the
form $(\x,\x')$ in the reachability relation that we have explored,
where $\x\in\reals^n$ and $\x'\in\reals^n$. Then, we wish to learn a
function described as $\x' = F(\vx, \vu)$.  The details can be found
in standard texts on learning and
statistics~\cite{friedman2001elements}.

% In this fashion, we can we can use OLS to approximate its trajectories
% of system $\System$ at fixed time step $\Delta$ by a discrete map $\x'
% = F(\hat{\x})$.

We note that linear regression tools are often able to provide an
$n\times 1$ vector $\vec{\delta}$ of error intervals, where the
$i^{th}$ entry of the form $[{\delta_i}_{\min},{\delta_i}_{\max}]$
indicates the best and worst possible error in computation of the
$i^{th}$ state in $\x'$.

Affine maps are poor approximations for arbitrary non-linear
functions. Hence, we use a collection of affine maps to approximate
the local behaviors (in state-space) of the system $\System$. This
results in a piece wise approximation, as described in the next
section.

\begin{example}
    For the~\exref{vdp}, for every abstract relation between two cells
    $C, C'$, linear regression analysis is performed on the respective
    set of trajectory segments, and the affine relation is computed.
    \figref{vdp-abs-paths} shows the cells and the trajectory segments, which
    are part of the data sets constructed using the $1$-relational
    modeling. Against each cell, its unique identifier (integer
    co-ordinate) is mentioned.
\end{example}
